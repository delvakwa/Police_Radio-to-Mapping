{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "#\n",
    "# This work is licensed under a Creative Commons \n",
    "# Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
    "#\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "#\n",
    "# The _B_roadcastify _AR_chive _T_oolkit\n",
    "#\n",
    "#   |\\/\\/\\/|  +------------------------+\n",
    "#   |      |  | Eat my shortwave, man. |\n",
    "#   |      |  +------------------------+\n",
    "#   | (o)(o)    /\n",
    "#   C      _)  /\n",
    "#    | ,___|\n",
    "#    |   /\n",
    "#   /____\\\n",
    "#  /      \\\n",
    "#\n",
    "#\n",
    "# Artwork source: https://www.asciiart.eu/cartoons/simpsons\n",
    "#\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Throughout the file, \"ATT\" is short for \"archiveTimes table\", which contains \n",
    "# archive entry information for the date selected in the navigation calendar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Imports\n",
    "#-----------------------------------------------------------------------------\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from IPython.display import clear_output\n",
    "from time import time as timer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import wait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Constants\n",
    "#-----------------------------------------------------------------------------\n",
    "_FEED_URL_STEM = 'https://www.broadcastify.com/listen/feed/'\n",
    "_ARCHIVE_FEED_STEM = 'https://m.broadcastify.com/archives/feed/'\n",
    "_ARCHIVE_DOWNLOAD_STEM = 'https://m.broadcastify.com/archives/id/'\n",
    "_LOGIN_URL = 'https://www.broadcastify.com/login/'\n",
    "_FIRST_URI_IN_ATT_XPATH = \"//a[contains(@href,'/archives/download/')]\"\n",
    "\n",
    "_FILE_REQUEST_WAIT = 5 # seconds\n",
    "_PAGE_REQUEST_WAIT = 2 # seconds\n",
    "_WEBDRIVER_PATH = '../assets/chromedriver' # to use Chrome in Selenium\n",
    "_MP3_OUT_PATH = '../audio_data/audio_files/mp3_files/'\n",
    "\n",
    "_MONTHS = ['','January', 'February', 'March',\n",
    "      'April', 'May', 'June',\n",
    "      'July', 'August', 'September',\n",
    "      'October', 'November', 'December']\n",
    "\n",
    "# Library-level variables\n",
    "ArchiveEntry = collections.namedtuple('ArchiveEntry',\n",
    "                                     'feed_id file_uri file_end_datetime mp3_url')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NavigatorException(Exception):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class _RequestThrottle:\n",
    "    \"\"\"\n",
    "    Limits the pace with which requests are sent to Broadcastify's servers\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.last_file_req = timer()\n",
    "        self.last_page_req = timer()\n",
    "        \n",
    "    def throttle(self, type='page'):\n",
    "        if type == 'page':\n",
    "            while not timer() - self.last_page_req >= _PAGE_REQUEST_WAIT:\n",
    "                pass\n",
    "            self.last_page_req = timer()\n",
    "        else:\n",
    "            while not timer() - self.last_file_req >= _FILE_REQUEST_WAIT:\n",
    "                pass\n",
    "            self.last_file_req = timer()     \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     65,
     72,
     82,
     92
    ]
   },
   "outputs": [],
   "source": [
    "class BroadcastifyArchive:\n",
    "    def __init__(self, feed_id, username=None, password=None, verbose=True):\n",
    "        \"\"\"\n",
    "        A container for Broadcastify feed archive data, and an enginge for retrieving\n",
    "        archive entry information and downloading the corresponding mp3 files. Initial-\n",
    "        izes to an empty container. Non-standard dependencies include:\n",
    "            - Selenium (pip install selenium)\n",
    "            - The WebDriver for Chrome (https://chromedriver.chromium.org/downloads)\n",
    "            - BeautifulSoup 4 (pip install beautifulsoup4)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feed_id : str\n",
    "            The unique feed identifier the container will represent, taken from \n",
    "            https://www.broadcastify.com/listen/feed/[feed_id].\n",
    "        username : str\n",
    "            The username for a valid Broadcastify premium account.\n",
    "        password : str\n",
    "            The password for a valid Broadcastify premium account.\n",
    "        verbose : bool\n",
    "            If True, the system will generate more verbose output during longer\n",
    "            operations.\n",
    "\n",
    "        Attributes & Properties\n",
    "        -----------------------\n",
    "        feed_id  : str\n",
    "        username : str\n",
    "        password : str\n",
    "            Same as init parameter\n",
    "        feed_url : str\n",
    "            Full https URL for the feed's main \"listen\" page.\n",
    "        archive_url : str\n",
    "            Full https URL for the feed's archive page.\n",
    "        entries : (ArchiveEntry named tuple)\n",
    "            Container for archive entry information.\n",
    "            feed_id : str\n",
    "                Same as feed_id parameter\n",
    "            file_uri : str\n",
    "                The unique ID for an individual archive file page, which corresponds to \n",
    "                a feed's transmissions over a ~30-minute period on a given date. Can be \n",
    "                used to find the mp3 file's individual download page. This page is pass-\n",
    "                word protected and limited to premium account holders.\n",
    "            file_end_date_time : str\n",
    "                Date and end time of the individual archive file.\n",
    "            mp3_url : str\n",
    "                The URL of the corresponding mp3 file.\n",
    "        earliest_date : datetime\n",
    "        latest_date   : datetime\n",
    "            The datetime of the earliest/latest archive entry currently in `entries`.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.feed_id = feed_id\n",
    "        self.feed_url = _FEED_URL_STEM + feed_id\n",
    "        self.archive_url = _ARCHIVE_FEED_STEM + feed_id\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.entries = []\n",
    "        self.earliest_date = None \n",
    "        self.latest_date = None\n",
    "        \n",
    "        self._an = None\n",
    "        self._verbose = verbose\n",
    "        \n",
    "    @property\n",
    "    def feed_id(self):\n",
    "        \"\"\"\n",
    "        Unique ID for the Broadcastify feed. Taken from \n",
    "        https://www.broadcastify.com/listen/feed/[feed_id]\n",
    "        \"\"\"\n",
    "        return self._feed_id\n",
    "    @feed_id.setter\n",
    "    def feed_id(self, value):\n",
    "        self._feed_id = value\n",
    "        self.entries = []\n",
    "        self.earliest_date = None\n",
    "        self.latest_date = None\n",
    "        self.feed_url = _FEED_URL_STEM + value\n",
    "        self.archive_url = _ARCHIVE_FEED_STEM + value\n",
    "        self._an = None\n",
    "\n",
    "    @property\n",
    "    def password(self):\n",
    "        \"\"\"\n",
    "        Password for Broadcastify premium account. Getting the property\n",
    "        will return a Boolean indicating whether the password has been set.\n",
    "        \"\"\"\n",
    "        if self._password:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    @password.setter\n",
    "    def password(self, value):\n",
    "        self._password = value\n",
    "\n",
    "    def build(self, days_back=0, rebuild=False): # 0 days back means the active day\n",
    "        \"\"\"\n",
    "        Build the archive entry data for the given archive specified by the\n",
    "        BroadcastifyArchive's feed_id. The major steps include:\n",
    "            - Navigate to the feed's archive page\n",
    "            - Scrape the archiveTimes Table (ATT) for each day in days_back\n",
    "            - Navigate to the password-protected file_uri to retrieve the\n",
    "              URL of the entry's mp3 file\n",
    "            - Populate the list of ArchiveEntry tuples for all retrieved data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        days_back : int (0 to 180)\n",
    "            The number of days before the current day to retrieve information for.\n",
    "            A value of `0` retrieves only archive entries corresponding to the current \n",
    "            day. Broadcastify archives go back only 180 days.\n",
    "        rebuild : bool\n",
    "            Specifies that existing data in the `entries` list should be erased and \n",
    "            overwritten with data newly fetched from Broadcastify.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if self.entries and not rebuild:\n",
    "            raise ValueError(f'Entries already exist for this BroadcastifyArchive. '\n",
    "                             f'To rebuild, specify rebuild=True when calling .build()')\n",
    "        \n",
    "        all_att_entries = []\n",
    "        counter = 1\n",
    "        \n",
    "        # Make sure days_back is an integer and non-negative\n",
    "        try:\n",
    "            days_back = int(days_back)\n",
    "            if days_back < 0: days_back = 0\n",
    "            days_back += 1\n",
    "        except (TypeError):\n",
    "            raise TypeError(\"The `days_back` parameter needs an integer between 0 and 180.\")\n",
    "        \n",
    "        if self._verbose: print('Starting the _ArchiveNavigator...')\n",
    "\n",
    "        # Instantiate the _ArchiveNavigator\n",
    "        self.an = _ArchiveNavigator(self.archive_url, self._verbose)\n",
    "        \n",
    "        # Add the current (zero-th) day's ATT entries \n",
    "        # (file_uri & file_end_date_time)\n",
    "        if self._verbose: print(f'Parsing day {counter} of {days_back}: {self.an.active_date}')\n",
    "        \n",
    "        all_att_entries = self.__parse_att(self.an.att_soup)\n",
    "        self.latest_date = all_att_entries[0][1]\n",
    "        self.earliest_date = all_att_entries[-1][1]\n",
    "\n",
    "        # For each day requested...\n",
    "        for day in range(1, days_back):\n",
    "            # If clicking the prior day takes us past the beginning of the archive,\n",
    "            # stop.\n",
    "            if not self.an.click_prior_day(): break\n",
    "            \n",
    "            counter += 1\n",
    "            if self._verbose: print(f'Parsing day {counter} of {days_back}: {self.an.active_date}')\n",
    "            \n",
    "            # Get the ATT entries (file_uri & file_end_date_time)\n",
    "            all_att_entries.extend(self.__parse_att(self.an.att_soup))\n",
    "            self.earliest_date = all_att_entries[-1][1]\n",
    "            \n",
    "        self.an.close_browser()\n",
    "        \n",
    "        # Iterate through ATT entries to\n",
    "        ##  - Get the mp3 URL\n",
    "        ##  - Build an ArchiveEntry, and append to the list\n",
    "        ## Instantiate the _DownloadNavigator\n",
    "        dn = _DownloadNavigator(login=True,\n",
    "                                username=self.username,\n",
    "                                password=self._password,\n",
    "                                verbose=self._verbose)\n",
    "        counter = 0\n",
    "        \n",
    "        ## Loop & build ArchiveEntry list\n",
    "        for uri, end_time in all_att_entries:\n",
    "            counter += 1\n",
    "            if self._verbose: print(f'Building ArchiveEntry list: {counter} of {len(all_att_entries)}')\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            mp3_soup = dn.get_download_soup(uri)\n",
    "            mp3_path = self.__parse_mp3_path(mp3_soup)\n",
    "\n",
    "            self.entries.append(ArchiveEntry(self.feed_id,\n",
    "                                             uri,\n",
    "                                             end_time,\n",
    "                                             mp3_path))\n",
    "\n",
    "        if self._verbose:\n",
    "            print(f'Archive build complete.')\n",
    "            print(self)\n",
    "            \n",
    "    def download(self, start=None, end=None, output_path=_MP3_OUT_PATH):\n",
    "        \"\"\"\n",
    "        Download mp3 files for the archive entries currently in the `entries`\n",
    "        list and between the start and end dates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start : datetime\n",
    "        end   : datetime\n",
    "            The earliest date for which to retrieve files\n",
    "        output_path : str (optional)\n",
    "            The local path to which archive entry mp3 files will be written. The\n",
    "            path must exist before calling the method. Defaults to \n",
    "            '../audio_data/audio_files/mp3_files/'.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        entries = self.entries\n",
    "        entries_to_pass = []\n",
    "        dn = _DownloadNavigator(login=False, verbose=self._verbose)\n",
    "        \n",
    "        if not start: start = datetime(1,1,1,0,0)\n",
    "        if not end: end = datetime(9999,12,31,0,0)\n",
    "\n",
    "        if self._verbose:\n",
    "            print(f'Retrieving list of ArchiveEntries...\\n'\n",
    "                  f' no earlier than {start}\\n'\n",
    "                  f' no later than   {end}')\n",
    "            \n",
    "        # Remove out-of-date-range entries from self.entries\n",
    "        entries_to_pass = [entry for entry in entries if \n",
    "                           entry.file_end_datetime >= start and\n",
    "                           entry.file_end_datetime <= end]\n",
    "        \n",
    "        if self._verbose:\n",
    "            print(f'\\n{len(entries_to_pass)} ArchiveEntries matched.')\n",
    "\n",
    "        # Pass them as a list to a _DownloadNavigator.get_archive_mp3s\n",
    "        dn.get_archive_mp3s(entries_to_pass, output_path)\n",
    "    \n",
    "    def __parse_att(self, att_soup):\n",
    "        \"\"\"\n",
    "        Generates Broadcastify archive file information from the `archiveTimes`\n",
    "        table (\"ATT\") on a feed's archive page. Returns a list of lists\n",
    "        containing two elements:\n",
    "            - The URI for the file, which can be used to find the file's\n",
    "              individual download page\n",
    "            - Date and end time of the transmission as a datetime object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        att_soup : bs4.BeautifulSoup\n",
    "            A BeautifulSoup object containing the ATT source code, obtained\n",
    "            from _ArchiveNavigator.att_soup\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set up a blank list to return\n",
    "        att_entries = []\n",
    "\n",
    "        # Loop through all rows of the table\n",
    "        for row in att_soup.find_all('tr'):    \n",
    "            # Grab the end time, contained in the row's second <td> tag\n",
    "            file_end_datetime = self.__get_entry_end_datetime(row.find_all('td')[1].text)\n",
    "            \n",
    "            # Grab the file ID\n",
    "            file_uri = row.find('a')['href'].split('/')[-1]\n",
    "\n",
    "            # Put the file date/time and URL leaf (as a list) into the list\n",
    "            att_entries.append([file_uri, file_end_datetime])\n",
    "        \n",
    "        return att_entries\n",
    "    \n",
    "    def __get_entry_end_datetime(self, time):\n",
    "        \"\"\"Convert the archive entry end time to datetime\"\"\"\n",
    "        hhmm = datetime.strptime(time, '%I:%M %p')\n",
    "        return datetime.combine(self.an.active_date, datetime.time(hhmm))\n",
    "        \n",
    "    def __parse_mp3_path(self, download_page_soup):\n",
    "        \"\"\"Parse the mp3 filepath from a BeautifulSoup of the download page\"\"\"\n",
    "        return download_page_soup.find('a', {'href': re.compile('.mp3')}).attrs['href']\n",
    "\n",
    "    def __repr__(self):\n",
    "        return(f'BroadcastifyArchive\\n'\n",
    "               f' ('\n",
    "               f'{len(self.entries)} ArchiveEntries\\n'\n",
    "               f'  start date: {str(self.earliest_date)}\\n'\n",
    "               f'  end date:   {str(self.latest_date)}\\n'\n",
    "               f'  feed_id = {self.feed_id}\\n'\n",
    "               f'  username = \"{self.username}\", pwd = [{self.password}]\\n'\n",
    "               f'  feed_url = \"{self.feed_url}\"\\n'\n",
    "               f'  archive_url = \"{self.archive_url}\"\\n'\n",
    "               f'  verbose = {self._verbose}'\n",
    "               f')')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class _ArchiveNavigator:\n",
    "    def __init__(self, url, verbose):\n",
    "        \"\"\"\n",
    "        Utility for navigating the archive feed.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            Full https URL for the feed's archive page.\n",
    "        verbose : bool\n",
    "            If True, the system will generate more verbose output during longer\n",
    "            operations.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.url = url\n",
    "        self.calendar_soup = None\n",
    "        self.att_soup = None\n",
    "        self.browser = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.active_date = None # currently displayed date\n",
    "        self.month_max_date = None # latest day in displayed month with archive entries\n",
    "        self.month_min_date = None # earliest day in displayed month with archive entries\n",
    "        \n",
    "        self.current_first_uri = None\n",
    "       \n",
    "        # Get initial page scrape & parse the calendar\n",
    "        self.open_browser()\n",
    "        self.__load_nav_page()\n",
    "        self.__scrape_nav_page()\n",
    "        self.__parse_calendar()\n",
    "        \n",
    "        self.archive_max_date = self.active_date\n",
    "        \n",
    "        # https://www.saltycrane.com/blog/2010/10/how-get-date-n-days-ago-python/\n",
    "        self.archive_min_date = self.archive_max_date - timedelta(days=181)\n",
    "        \n",
    "    def click_prior_day(self):\n",
    "        # calculate the prior day\n",
    "        prior_day = self.active_date - timedelta(days=1)\n",
    "        \n",
    "        # would this take us past the archive? if so, stop.\n",
    "        if prior_day < self.archive_min_date:\n",
    "            return False\n",
    "        \n",
    "        # is the prior day in the previous month? set the xpath class appropriately\n",
    "        if prior_day < self.month_min_date:\n",
    "            xpath_class = 'old day'\n",
    "        else:\n",
    "            xpath_class = 'day'\n",
    "\n",
    "        xpath_day = prior_day.day\n",
    "        \n",
    "        self.__check_browser()\n",
    "        \n",
    "        # click the day before the currently displayed day\n",
    "        calendar_day = self.browser.find_element_by_xpath(\n",
    "                        f\"//td[@class='{xpath_class}' \"\n",
    "                        f\"and contains(text(), '{xpath_day}')]\")\n",
    "            # https://stackoverflow.com/questions/2009268/how-to-write-an-xpath-query-to-match-two-attributes\n",
    "        calendar_day.click()\n",
    "\n",
    "        # refresh soup & re-parse calendar\n",
    "        self.__scrape_nav_page()\n",
    "        self.__parse_calendar()\n",
    "        \n",
    "        return self.active_date\n",
    "    \n",
    "    def __load_nav_page(self):\n",
    "        if self.verbose: print('Loading navigation page...')\n",
    "        self.__check_browser()\n",
    "\n",
    "        # Browse to feed archive page\n",
    "        self.browser.get(self.url)\n",
    "        \n",
    "        # Wait for page to render\n",
    "        element = WebDriverWait(self.browser, 10).until(\n",
    "                  EC.presence_of_element_located((By.CLASS_NAME, \n",
    "                                                  \"cursor-link\")))\n",
    "        \n",
    "        # Get current_first_uri, if none populated\n",
    "        if not self.current_first_uri:\n",
    "            self.current_first_uri = self.__get_current_first_uri()\n",
    "    \n",
    "    def __scrape_nav_page(self):\n",
    "        if self.verbose: print('Scraping navigation page...')\n",
    "        self.__check_browser()\n",
    "\n",
    "        # Wait for page to render\n",
    "        element = WebDriverWait(self.browser, 10).until_not(\n",
    "                    EC.text_to_be_present_in_element((By.XPATH, _FIRST_URI_IN_ATT_XPATH), \n",
    "                                                      self.current_first_uri))\n",
    "\n",
    "        self.current_first_uri = self.__get_current_first_uri()\n",
    "        \n",
    "        # Scrape page content\n",
    "        soup = BeautifulSoup(self.browser.page_source, 'lxml')\n",
    "\n",
    "        # Isolate the calendar and the archiveTimes table\n",
    "        self.calendar_soup = soup.find('table', \n",
    "                                       {'class': 'table-condensed'})\n",
    "        self.att_soup = soup.find('table', \n",
    "                                  attrs={'id': 'archiveTimes'}\n",
    "                                  ).find('tbody')\n",
    "        \n",
    "    def __parse_calendar(self):\n",
    "        \"\"\"\n",
    "        Uses a bs4 ResultSet of the <td> tags representing days currently displayed\n",
    "        on the calendar to set calendarattributes. Items have the format of \n",
    "        `<td class=\"[class]\">[D]</td>` where \n",
    "         - [D] is the one- or two-digit day (as a string) and\n",
    "         - [class] is one of\n",
    "             \"old day\"          = a day with archives but in a prior month (clicking\n",
    "                                  will refresh the calendar)\n",
    "             \"day\"              = a past day in the current month\n",
    "             \"active day\"       = the day currently displayed in the ATT\n",
    "             \"disabled day\"     = a day for which no archive is available in a month\n",
    "                                  (past or future) that has other days with archives. \n",
    "                                  For example, if today is July 27, July 28-31 will \n",
    "                                  be disabled days, as will January 1-26 (since the \n",
    "                                  archive goes back only 180 days). December 31 would\n",
    "                                  be an \"old disabled day\".\n",
    "                                  past month for which archives are no longer available\n",
    "             \"new disabled day\" = a day in a future month\n",
    "             \"old disabled day\" = see explanation in \"disabled day\"\n",
    "         \n",
    "        \"\"\"\n",
    "        if self.verbose: print('Parsing calendar...')\n",
    "        \n",
    "        # Get the tags representing the days currently displayed on the calendar\n",
    "        days_on_calendar = self.calendar_soup.find_all('td')\n",
    "        \n",
    "        # Get the month & year currently displayed\n",
    "        month, year = self.calendar_soup.find('th', \n",
    "                                              {'class': 'datepicker-switch'}\n",
    "                                              ).text.split(' ')\n",
    "        \n",
    "        displayed_month = _MONTHS.index(month)\n",
    "        displayed_year = int(year)\n",
    "        \n",
    "        # Parse the various calendar attributes\n",
    "        active_day = int([day.text for day in days_on_calendar\n",
    "                           if (day['class'][0] == 'active')][0])\n",
    "        \n",
    "        month_max_day = int([day.text for day in days_on_calendar\n",
    "                              if (day['class'][0] == 'day') or\n",
    "                                 (day['class'][0] == 'active')][::-1][0])\n",
    "        \n",
    "        month_min_day = int(self.__parse_month_min_day(days_on_calendar))\n",
    "        \n",
    "        # Set class attributes\n",
    "        self.active_date = date(displayed_year, displayed_month, active_day)        \n",
    "        self.month_min_date = date(displayed_year, displayed_month, month_min_day)\n",
    "        self.month_max_date = date(displayed_year, displayed_month, month_max_day)\n",
    "        \n",
    "    def __parse_month_min_day(self, days_on_calendar):\n",
    "        \"\"\"Parse the lowest valid day in the displayed month\"\"\"\n",
    "        disabled_found = False\n",
    "        for day in days_on_calendar:\n",
    "            if day['class'][0] == 'disabled':\n",
    "                disabled_found = True\n",
    "            elif day['class'][0] in 'day active'.split():\n",
    "                return day.text\n",
    "            elif day['class'][0] != 'old' and disabled_found:\n",
    "                return day.text\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def __get_current_first_uri(self):\n",
    "        return self.browser.find_element_by_xpath(\n",
    "                    _FIRST_URI_IN_ATT_XPATH\n",
    "                    ).get_attribute('href').split('/')[-1]\n",
    "        \n",
    "    def open_browser(self):\n",
    "        if self.verbose: print('Opening browser...')\n",
    "\n",
    "        # Make Chrome invisible, comment if you want to see it in action...\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        # Launch Chrome\n",
    "        self.browser = webdriver.Chrome(_WEBDRIVER_PATH, chrome_options=options)\n",
    "\n",
    "    def close_browser(self):\n",
    "        if self.verbose: print('Closing browser...')\n",
    "        self.browser.quit()\n",
    "\n",
    "    def __check_browser(self):\n",
    "        if not self.browser:\n",
    "            raise NavigatorException(\"Please open a browser. And remember to close it when you're done.\")\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return(f'_ArchiveNavigator(URL: {self.url}, '\n",
    "               f'Currently Displayed: {str(self.active_date)}, '\n",
    "               f'Max Day: {str(self.archive_max_date)}, '\n",
    "               f'Min Day: {str(self.archive_min_date)}, ')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class _DownloadNavigator:\n",
    "    def __init__(self, login=False, username=None, password=None, verbose=False):\n",
    "        self.download_page_soup = None\n",
    "        self.current_archive_id = None\n",
    "        self.verbose = verbose\n",
    "        self.throttle = t = _RequestThrottle()\n",
    "        self.session = s = requests.Session()\n",
    "        self.login = l = login\n",
    "        \n",
    "        # Set post parameters\n",
    "        login_data = {\n",
    "            'username': username,\n",
    "            'password': password,\n",
    "            'action': 'auth',\n",
    "            'redirect': '/'\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) ' +\n",
    "                          'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/' +\n",
    "                          '75.0.3770.142 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        if l:\n",
    "            if not username or not password:\n",
    "                raise NavigatorException(\"If login=True, login credentials must be supplied.\")\n",
    "                \n",
    "            t.throttle()\n",
    "            r = s.post(_LOGIN_URL, data=login_data, headers=headers)\n",
    "\n",
    "            if r.status_code != 200:\n",
    "                raise ConnectionError(f'Problem connecting: {r.status_code}')\n",
    "        \n",
    "    def get_download_soup(self, archive_id):\n",
    "        self.current_archive_id = archive_id\n",
    "        s = self.session\n",
    "        t = self.throttle\n",
    "        \n",
    "        t.throttle()\n",
    "        r = s.get(_ARCHIVE_DOWNLOAD_STEM + archive_id)\n",
    "        if r.status_code != 200:\n",
    "            raise ConnectionError(f'Problem connecting: {r.status_code}')\n",
    "                                  \n",
    "        self.download_page_soup = BeautifulSoup(r.text, 'lxml')       \n",
    "\n",
    "        return self.download_page_soup        \n",
    "\n",
    "    def get_archive_mp3s(self, archive_entries, filepath):\n",
    "        start = timer()\n",
    "        \n",
    "        for file in archive_entries:\n",
    "            feed_id =  file.feed_id\n",
    "            archive_uri = file.file_uri\n",
    "            file_date = self.__format_entry_date(file.file_end_datetime)\n",
    "            file_url = file.mp3_url\n",
    "   \n",
    "            # Build the path for saving the downloaded .mp3\n",
    "            out_file_name = filepath + '-'.join([feed_id, file_date]) + '.mp3'\n",
    "\n",
    "            print(f'Downloading {archive_entries.index(file) + 1} of {len(archive_entries)}')\n",
    "            if self.verbose:\n",
    "                print(f'\\tfrom {file_url}')\n",
    "                print(f'\\tto {out_file_name}')\n",
    "            \n",
    "            self.throttle.throttle('file')\n",
    "            self.__fetch_mp3([out_file_name, file_url])\n",
    "            \n",
    "        duration = timer() - start\n",
    "\n",
    "        if len(archive_entries) > 0: print('\\nDownloads complete.')\n",
    "        if self.verbose:\n",
    "            print(f'\\nRetrieved {len(archive_entries)} files in {round(duration,4)} seconds.')\n",
    "    \n",
    "    def __fetch_mp3(self, entry):\n",
    "        # h/t https://markhneedham.com/blog/2018/07/15/python-parallel-download-files-requests/\n",
    "        path, uri = entry\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            r = requests.get(uri, stream=True)\n",
    "            if r.status_code == 200:\n",
    "                with open(path, 'wb') as f:\n",
    "                    for chunk in r:\n",
    "                        f.write(chunk)\n",
    "        return path\n",
    "\n",
    "    def __format_entry_date(self, date):\n",
    "        # Format the ArchiveEntry end time as YYYYMMDD-HHMM\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "        day = date.day\n",
    "        hour = date.hour\n",
    "        minute = date.minute\n",
    "        \n",
    "        return '-'.join([str(year) + str(month).zfill(2) + str(day).zfill(2), \n",
    "                         str(hour).zfill(2) + str(minute).zfill(2)])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(f'_DownloadNavigator(Current Archive: {self.current_archive_id})')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_FEED_ID = '763' # Sgt. Peterson, Chicago Police (top listened-to feed!)\n",
    "# TEST_FEED_ID = '14439' # Travis County, Austin, TX\n",
    "# TEST_FEED_ID = '18812' # Fulton County, Atlanta, GA\n",
    "# TEST_FEED_ID = '19807' # Lackawanna County, Scranton, PA\n",
    "\n",
    "# TEST_DOWNLOAD_ID = '774426456'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = BroadcastifyArchive(TEST_FEED_ID, USERNAME, PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba.build(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ba.download(start=datetime(2019,7,31,14,0), end=datetime(2019,8,1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sample_archive_entries.pkl', 'rb') as pickle_in:\n",
    "    sample_archive_entries = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_archive_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export pickle\n",
    "with open('../data/sample_archive_entries.pkl','wb') as pickle_out:\n",
    "    pickle.dump(sample_archive_entries, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = 'cwchiu'\n",
    "PASSWORD = 'datascientists'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "555px",
    "left": "918px",
    "right": "20px",
    "top": "41px",
    "width": "514px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
