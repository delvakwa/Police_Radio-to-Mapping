{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from IPython.display import clear_output\n",
    "from time import time as timer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import wait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughout, \"att\" is short for \"archiveTimes table\", which contains archive \n",
    "# entry info for the date selected in the calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "FEED_URL_STEM = 'https://www.broadcastify.com/listen/feed/'\n",
    "ARCHIVE_FEED_STEM = 'https://m.broadcastify.com/archives/feed/'\n",
    "ARCHIVE_DOWNLOAD_STEM = 'https://m.broadcastify.com/archives/id/'\n",
    "LOGIN_URL = 'https://www.broadcastify.com/login/'\n",
    "\n",
    "WEBDRIVER_PATH = '../assets/chromedriver'\n",
    "MP3_OUT_PATH = '../audio_data/audio_files/mp3_files/'\n",
    "\n",
    "FIRST_URI_IN_ATT_XPATH = \"//a[contains(@href,'/archives/download/')]\"\n",
    "\n",
    "FILE_REQUEST_WAIT = 5 # seconds\n",
    "PAGE_REQUEST_WAIT = 2 # seconds\n",
    "\n",
    "USERNAME = 'cwchiu'\n",
    "PASSWORD = 'datascientists'\n",
    "\n",
    "MONTHS = ['','January', 'February', 'March',\n",
    "      'April', 'May', 'June',\n",
    "      'July', 'August', 'September',\n",
    "      'October', 'November', 'December']\n",
    "\n",
    "# Library-level variables\n",
    "ArchiveEntry = collections.namedtuple('ArchiveEntry',\n",
    "                                     'feed_id file_uri file_end_datetime mp3_url')\n",
    "\"\"\"\n",
    "file_uri : str\n",
    "    The unique ID for an individual archive file, which corresponds to a feed's \n",
    "    transmission over a ~30-minute period on a given date. Can be used to find \n",
    "    the file's individual download page\n",
    "file_end_date_time : str\n",
    "    Date and end time of the transmission in the format YYYYMMDD-HHMM, on a \n",
    "    24-hour clock\n",
    "mp3_url : str\n",
    "    The URL of the corresponding mp3 file\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_request_wait = max(FILE_REQUEST_WAIT, PAGE_REQUEST_WAIT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "class RequestThrottle:\n",
    "    def __init__(self):\n",
    "        self.last_file_req = timer()\n",
    "        self.last_page_req = timer()\n",
    "        \n",
    "    def throttle(self, type='page'):\n",
    "        if type == 'page':\n",
    "            while not timer() - self.last_page_req >= PAGE_REQUEST_WAIT:\n",
    "                pass\n",
    "            self.last_page_req = timer()\n",
    "        else:\n",
    "            while not timer() - self.last_file_req >= FILE_REQUEST_WAIT:\n",
    "                pass\n",
    "            self.last_file_req = timer()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NoActiveBrowser(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     1,
     15,
     127,
     132,
     136
    ]
   },
   "outputs": [],
   "source": [
    "class BroadcastifyArchive:\n",
    "    def __init__(self, feed_id, username=None, password=None, verbose=1):\n",
    "        # Attributes\n",
    "        self.id = feed_id\n",
    "        self.url = ARCHIVE_FEED_STEM + feed_id\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.entries = [] # list of ArchiveEntry objects\n",
    "        self.earliest_date = None \n",
    "        self.latest_date = None\n",
    "        self.an = None\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.feed_url = FEED_URL_STEM + feed_id\n",
    "\n",
    "    def build(self, days_back=0): # 0 days back means the active day\n",
    "        \n",
    "        all_att_entries = []\n",
    "        counter = 1\n",
    "        \n",
    "        # Make sure days_back is an integer and non-negative\n",
    "        try:\n",
    "            days_back = int(days_back)\n",
    "            if days_back < 0: days_back = 0\n",
    "            days_back += 1\n",
    "        except (TypeError):\n",
    "            raise TypeError(\"The `days_back` parameter needs an integer between 0 and 180.\")\n",
    "        \n",
    "        if self.verbose: print('Starting the ArchiveNavigator...')\n",
    "\n",
    "        # Instantiate the ArchiveNavigator\n",
    "        self.an = ArchiveNavigator(self.url, self.verbose)\n",
    "        \n",
    "        # Add the current (zero-th) day's archiveTimes table entries \n",
    "        # (file_uri & file_end_date_time)\n",
    "        if self.verbose: print(f'Parsing day {counter} of {days_back}: {self.an.active_date}')\n",
    "        all_att_entries = self.__parse_att(self.an.att_soup)\n",
    "        self.latest_date = all_att_entries[0][1]\n",
    "        self.earliest_date = all_att_entries[-1][1]\n",
    "\n",
    "        # For each day requested...\n",
    "        for day in range(1, days_back):\n",
    "            # If clicking the prior day takes us past the beginning of the archive,\n",
    "            # stop.\n",
    "            if not self.an.click_prior_day(): break\n",
    "            \n",
    "            counter += 1\n",
    "            if self.verbose: print(f'Parsing day {counter} of {days_back}: {self.an.active_date}')\n",
    "            \n",
    "            # Get the archiveTimes table entries (file_uri & file_end_date_time)\n",
    "            all_att_entries.extend(self.__parse_att(self.an.att_soup))\n",
    "            self.earliest_date = all_att_entries[-1][1]\n",
    "            \n",
    "        self.an.close_browser()\n",
    "        \n",
    "        # Iterate through att entries to\n",
    "        ##  - Get the mp3 URL\n",
    "        ##  - Build an ArchiveEntry, and append to the list\n",
    "        ## Instantiate the DownloadNavigator\n",
    "        dn = DownloadNavigator(login=True, verbose=self.verbose)\n",
    "        counter = 0\n",
    "        \n",
    "        ## Loop & build ArchiveEntry list\n",
    "        for uri, end_time in all_att_entries:\n",
    "            counter += 1\n",
    "            if self.verbose: print(f'Building ArchiveEntry list: {counter} of {len(all_att_entries)}')\n",
    "            clear_output(wait=True)\n",
    "            self.entries.append(ArchiveEntry(self.id,\n",
    "                                             uri,\n",
    "                                             end_time,\n",
    "                                             self.__parse_mp3_path(\n",
    "                                             dn.get_download_soup(uri))))\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Archive build complete.')\n",
    "            print(self)\n",
    "            \n",
    "    def download(self, start=None, end=None):\n",
    "        entries = self.archive_entries\n",
    "        entries_to_pass = []\n",
    "        dn = DownloadNavigator(login=False)\n",
    "        \n",
    "        if not start: start = datetime(1,1,1,0,0)\n",
    "        if not end: end = datetime(9999,12,31,0,0)\n",
    "        \n",
    "        # Remove out-of-date-range entries from self.entries\n",
    "        entries_to_pass = [entry for entry in entries if \n",
    "                           entry.file_end_datetime >= start and\n",
    "                           entry.file_end_datetime <= end]\n",
    "        \n",
    "        # Pass them as a list to a DownloadNavigator.get_archive_mp3s\n",
    "        dn.get_archive_mp3s(entries_to_pass)\n",
    "    \n",
    "    def __parse_att(self, att_soup):\n",
    "        \"\"\"\n",
    "        Generates Broadcastify archive file information from the `archiveTimes`\n",
    "        table (\"ATT\") on a feed's archive page. Returns a list of lists\n",
    "        containing two elements:\n",
    "            - The URI for the file, which can be used to find the file's\n",
    "              individual download page\n",
    "            - Date and end time of the transmission as a datetime object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        att_soup : bs4.BeautifulSoup\n",
    "            A BeautifulSoup object containing the ATT source code, obtained\n",
    "            from ArchiveNavigator.att_soup\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set up a blank list to return\n",
    "        att_entries = []\n",
    "\n",
    "        # Loop through all rows of the table\n",
    "        for row in att_soup.find_all('tr'):    \n",
    "            # Grab the end time, contained in the row's second <td> tag\n",
    "            file_end_datetime = self.__get_entry_end_datetime(row.find_all('td')[1].text)\n",
    "            \n",
    "            # Grab the file ID\n",
    "            file_uri = row.find('a')['href'].split('/')[-1]\n",
    "\n",
    "            # Put the file date/time and URL leaf (as a list) into the list\n",
    "            att_entries.append([file_uri, file_end_datetime])\n",
    "        \n",
    "        return att_entries\n",
    "    \n",
    "    def __get_entry_end_datetime(self, time):\n",
    "        \"\"\"Convert the archive entry end time to datetime\"\"\"\n",
    "        clock = datetime.strptime(time, '%I:%M %p')\n",
    "        return datetime.combine(self.an.active_date, datetime.time(clock))\n",
    "        \n",
    "    def __parse_mp3_path(self, download_page_soup):\n",
    "        \"\"\"Parse the mp3 filepath from a BeautifulSoup of the download page\"\"\"\n",
    "        return download_page_soup.find('a', {'href': re.compile('.mp3')}).attrs['href']\n",
    "\n",
    "    def __repr__(self):\n",
    "        return(f'BroadcastifyArchive({len(self.entries)} ArchiveEntries; '\n",
    "               f'from {str(self.latest_date)} to {str(self.earliest_date)}. '\n",
    "               f'{self.url})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     93,
     144,
     166,
     170,
     174
    ]
   },
   "outputs": [],
   "source": [
    "class ArchiveNavigator:\n",
    "    def __init__(self, url, verbose):\n",
    "        self.url = url\n",
    "        self.calendar_soup = None\n",
    "        self.att_soup = None\n",
    "        self.browser = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.active_date = None # currently displayed date\n",
    "        self.month_max_date = None # latest day in displayed month with archive entries\n",
    "        self.month_min_date = None # earliest day in displayed month with archive entries\n",
    "        \n",
    "        self.current_first_uri = None\n",
    "       \n",
    "        # Get initial page scrape & parse the calendar\n",
    "        self.open_browser()\n",
    "        self.__load_nav_page()\n",
    "        self.__scrape_nav_page()\n",
    "        self.__parse_calendar()\n",
    "        \n",
    "        self.archive_max_date = self.active_date\n",
    "        \n",
    "        # https://www.saltycrane.com/blog/2010/10/how-get-date-n-days-ago-python/\n",
    "        self.archive_min_date = self.archive_max_date - timedelta(days=181)\n",
    "        \n",
    "    def click_prior_day(self):\n",
    "        # calculate the prior day\n",
    "        prior_day = self.active_date - timedelta(days=1)\n",
    "        \n",
    "        # would this take us past the archive? if so, stop.\n",
    "        if prior_day < self.archive_min_date:\n",
    "            return False\n",
    "        \n",
    "        # is the prior day in the previous month? set the xpath class appropriately\n",
    "        if prior_day < self.month_min_date:\n",
    "            xpath_class = 'old day'\n",
    "        else:\n",
    "            xpath_class = 'day'\n",
    "\n",
    "        xpath_day = prior_day.day\n",
    "        \n",
    "        self.__check_browser()\n",
    "        \n",
    "        # click the day before the currently displayed day\n",
    "        calendar_day = self.browser.find_element_by_xpath(\n",
    "                        f\"//td[@class='{xpath_class}' \"\n",
    "                        f\"and contains(text(), '{xpath_day}')]\")\n",
    "            # https://stackoverflow.com/questions/2009268/how-to-write-an-xpath-query-to-match-two-attributes\n",
    "        calendar_day.click()\n",
    "\n",
    "        # refresh soup & re-parse calendar\n",
    "        self.__scrape_nav_page()\n",
    "        self.__parse_calendar()\n",
    "        \n",
    "        return self.active_date\n",
    "    \n",
    "    def __load_nav_page(self):\n",
    "        if self.verbose: print('Loading navigation page...')\n",
    "        self.__check_browser()\n",
    "\n",
    "        # Browse to feed archive page\n",
    "        self.browser.get(self.url)\n",
    "        \n",
    "        # Wait for page to render\n",
    "        element = WebDriverWait(self.browser, 10).until(\n",
    "                  EC.presence_of_element_located((By.CLASS_NAME, \n",
    "                                                  \"cursor-link\")))\n",
    "        \n",
    "        # Get current_first_uri, if none populated\n",
    "        if not self.current_first_uri:\n",
    "            self.current_first_uri = self.__get_current_first_uri()\n",
    "    \n",
    "    def __scrape_nav_page(self):\n",
    "        if self.verbose: print('Scraping navigation page...')\n",
    "        self.__check_browser()\n",
    "\n",
    "        # Wait for page to render\n",
    "        element = WebDriverWait(self.browser, 10).until_not(\n",
    "                    EC.text_to_be_present_in_element((By.XPATH, FIRST_URI_IN_ATT_XPATH), \n",
    "                                                      self.current_first_uri))\n",
    "\n",
    "        self.current_first_uri = self.__get_current_first_uri()\n",
    "        \n",
    "        # Scrape page content\n",
    "        soup = BeautifulSoup(self.browser.page_source, 'lxml')\n",
    "\n",
    "        # Isolate the calendar and the archiveTimes table\n",
    "        self.calendar_soup = soup.find('table', \n",
    "                                       {'class': 'table-condensed'})\n",
    "        self.att_soup = soup.find('table', \n",
    "                                  attrs={'id': 'archiveTimes'}\n",
    "                                  ).find('tbody')\n",
    "        \n",
    "    def __parse_calendar(self):\n",
    "        \"\"\"\n",
    "        Uses a bs4 ResultSet of the <td> tags representing days currently displayed\n",
    "        on the calendar to set calendarattributes. Items have the format of \n",
    "        `<td class=\"[class]\">[D]</td>` where \n",
    "         - [D] is the one- or two-digit day (as a string) and\n",
    "         - [class] is one of\n",
    "             \"old day\"          = a day with archives but in a prior month (clicking\n",
    "                                  will refresh the calendar)\n",
    "             \"day\"              = a past day in the current month\n",
    "             \"active day\"       = the day currently displayed in the archiveTimes \n",
    "                                  table\n",
    "             \"disabled day\"     = a day for which no archive is available in a month\n",
    "                                  (past or future) that has other days with archives. \n",
    "                                  For example, if today is July 27, July 28-31 will \n",
    "                                  be disabled days, as will January 1-26 (since the \n",
    "                                  archive goes back only 180 days). December 31 would\n",
    "                                  be an \"old disabled day\".\n",
    "                                  past month for which archives are no longer available\n",
    "             \"new disabled day\" = a day in a future month\n",
    "             \"old disabled day\" = see explanation in \"disabled day\"\n",
    "         \n",
    "        \"\"\"\n",
    "        if self.verbose: print('Parsing calendar...')\n",
    "        \n",
    "        # Get the tags representing the days currently displayed on the calendar\n",
    "        days_on_calendar = self.calendar_soup.find_all('td')\n",
    "        \n",
    "        # Get the month & year currently displayed\n",
    "        month, year = self.calendar_soup.find('th', \n",
    "                                              {'class': 'datepicker-switch'}\n",
    "                                              ).text.split(' ')\n",
    "        \n",
    "        displayed_month = MONTHS.index(month)\n",
    "        displayed_year = int(year)\n",
    "        \n",
    "        # Parse the various calendar attributes\n",
    "        active_day = int([day.text for day in days_on_calendar\n",
    "                           if (day['class'][0] == 'active')][0])\n",
    "        \n",
    "        month_max_day = int([day.text for day in days_on_calendar\n",
    "                              if (day['class'][0] == 'day') or\n",
    "                                 (day['class'][0] == 'active')][::-1][0])\n",
    "        \n",
    "        month_min_day = int(self.__parse_month_min_day(days_on_calendar))\n",
    "        \n",
    "        # Set class attributes\n",
    "        self.active_date = date(displayed_year, displayed_month, active_day)        \n",
    "        self.month_min_date = date(displayed_year, displayed_month, month_min_day)\n",
    "        self.month_max_date = date(displayed_year, displayed_month, month_max_day)\n",
    "        \n",
    "    def __parse_month_min_day(self, days_on_calendar):\n",
    "        \"\"\"Parse the lowest valid day in the displayed month\"\"\"\n",
    "        disabled_found = False\n",
    "        for day in days_on_calendar:\n",
    "            if day['class'][0] == 'disabled':\n",
    "                disabled_found = True\n",
    "            elif day['class'][0] in 'day active'.split():\n",
    "                return day.text\n",
    "            elif day['class'][0] != 'old' and disabled_found:\n",
    "                return day.text\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def __get_current_first_uri(self):\n",
    "        return self.browser.find_element_by_xpath(\n",
    "                    FIRST_URI_IN_ATT_XPATH\n",
    "                    ).get_attribute('href').split('/')[-1]\n",
    "        \n",
    "    def open_browser(self):\n",
    "        if self.verbose: print('Opening browser...')\n",
    "        self.browser = webdriver.Chrome(WEBDRIVER_PATH)\n",
    "\n",
    "    def close_browser(self):\n",
    "        if self.verbose: print('Closing browser...')\n",
    "        self.browser.quit()\n",
    "\n",
    "    def __check_browser(self):\n",
    "        if not self.browser:\n",
    "            raise NoActiveBrowser(\"Please open a browser. And remember to close it when you're done.\")\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return(f'ArchiveNavigator(URL: {self.url}, '\n",
    "               f'Currently Displayed: {str(self.active_date)}, '\n",
    "               f'Max Day: {str(self.archive_max_date)}, '\n",
    "               f'Min Day: {str(self.archive_min_date)}, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DownloadNavigator():\n",
    "    def __init__(self, login=False, verbose=False):\n",
    "        self.download_page_soup = None\n",
    "        self.current_archive_id = None\n",
    "        self.verbose = verbose\n",
    "        self.throttle = t = RequestThrottle()\n",
    "        self.session = s = requests.Session()\n",
    "        self.login = l = login\n",
    "        \n",
    "        # Set post parameters\n",
    "        login_data = {\n",
    "            'username': USERNAME,\n",
    "            'password': PASSWORD,\n",
    "            'action': 'auth',\n",
    "            'redirect': '/'\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) ' +\n",
    "                          'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/' +\n",
    "                          '75.0.3770.142 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        if l:\n",
    "            t.throttle()\n",
    "            r = s.post(LOGIN_URL, data=login_data, headers=headers)\n",
    "\n",
    "            if r.status_code != 200:\n",
    "                raise ConnectionError(f'Problem connecting: {r.status_code}')\n",
    "        \n",
    "    def get_download_soup(self, archive_id):\n",
    "        self.current_archive_id = archive_id\n",
    "        s = self.session\n",
    "        t = self.throttle\n",
    "        \n",
    "        t.throttle()\n",
    "        r = s.get('https://m.broadcastify.com/archives/id/' + archive_id)\n",
    "        if r.status_code != 200:\n",
    "            raise ConnectionError(f'Problem connecting: {r.status_code}')\n",
    "                                  \n",
    "        self.download_page_soup = BeautifulSoup(r.text, 'lxml')       \n",
    "\n",
    "        return self.download_page_soup        \n",
    "\n",
    "    def get_archive_mp3s(self, archive_entries):\n",
    "        start = timer()\n",
    "   \n",
    "        for file in archive_entries:\n",
    "            feed_id =  file.feed_id\n",
    "            archive_uri = file.file_uri\n",
    "            file_date = self.__format_entry_date(file.file_end_datetime)\n",
    "            file_url = file.mp3_url\n",
    "   \n",
    "            # Build the path for saving the downloaded .mp3\n",
    "            out_file_name = MP3_OUT_PATH + '-'.join([feed_id, file_date]) + '.mp3'\n",
    "\n",
    "            print(f'Downloading {archive_entries.index(file) + 1} of {len(archive_entries)}')\n",
    "            if self.verbose:\n",
    "                print(f'\\tfrom {file_url}')\n",
    "                print(f'\\tto {out_file_name}')\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            self.throttle.throttle('file')\n",
    "            self.__fetch_mp3([out_file_name, file_url])\n",
    "\n",
    "        duration = timer() - start\n",
    "\n",
    "        print('Downloads complete.')\n",
    "        if self.verbose:\n",
    "            print(f'Retrieved {len(out_file_name)} files in {duration} seconds.')\n",
    "    \n",
    "    def __fetch_mp3(self, entry):\n",
    "        # h/t https://markhneedham.com/blog/2018/07/15/python-parallel-download-files-requests/\n",
    "        path, uri = entry\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            r = requests.get(uri, stream=True)\n",
    "            if r.status_code == 200:\n",
    "                with open(path, 'wb') as f:\n",
    "                    for chunk in r:\n",
    "                        f.write(chunk)\n",
    "        return path\n",
    "\n",
    "    def __format_entry_date(self, date):\n",
    "        # Format the ArchiveEntry end time as YYYYMMDD-HHMM\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "        day = date.day\n",
    "        hour = date.hour\n",
    "        minute = date.minute\n",
    "        \n",
    "        return '-'.join([str(year) + str(month).zfill(2) + str(day).zfill(2), \n",
    "                         str(hour).zfill(2) + str(minute).zfill(2)])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(f'DownloadNavigator(Current Archive: {self.current_archive_id})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# class ArchiveTimesTable:\n",
    "#     def __init__(self, parent, archive_page_soup):\n",
    "#         self.parent = parent\n",
    "#         self.soup = archive_page_soup\n",
    "#         self.table_entries = self.__parse_entries()\n",
    "        \n",
    "#         # Properties\n",
    "#         @property\n",
    "#         def table_entries(self):\n",
    "#             \"\"\"Username for Broadcastify premium account.\"\"\"\n",
    "#             print('Inside property construct.')\n",
    "#             return self.table_entries\n",
    "#         @table_entries.setter\n",
    "#         def table_entries(self, value):\n",
    "#             self.table_entries = value\n",
    "#             print('Inside property construct.')\n",
    "        \n",
    "# #     def __parse_entries(self):\n",
    "# #         \"\"\"\n",
    "# #         Generates a list of Broadcastify archive file information from\n",
    "# #         the `archiveTimes` table on a feed's archive page. Each item in\n",
    "# #         the list is a list of two elements:\n",
    "# #             - The unique ID for the file, which can be used to find the file's\n",
    "# #               individual download page\n",
    "# #             - Date and end time of the transmission in the format YYYYMMDD-HHMM,\n",
    "# #               on a 24-hour clock\n",
    "\n",
    "# #         Parameters\n",
    "# #         ----------\n",
    "# #         self.soup : bs4.BeautifulSoup\n",
    "# #             A BeautifulSoup object containing the feed archive page source code, \n",
    "# #             e.g. from https://m.broadcastify.com/archives/feed/[feed_id]\n",
    "\n",
    "\n",
    "# #         \"\"\"\n",
    "# #         # Set up a blank list to return\n",
    "# #         table_entry_builder = []\n",
    "\n",
    "# #         # Isolate the `archive_times` table body\n",
    "# #         archive_times = self.soup.find('table', attrs={'id': 'archiveTimes'}).find('tbody')\n",
    "\n",
    "# #         # Find the date of transmission of the archived files\n",
    "# #         archive_date = self.__format_archive_date()\n",
    "\n",
    "# #         # Loop through all rows of the table\n",
    "# #         for row in archive_times.find_all('tr'):\n",
    "\n",
    "# #             # Grab the end time, contained in the row's second <td> tag\n",
    "# #             file_end_time = self.__time_to_hhmm(row.find_all('td')[1].text) \n",
    "\n",
    "# #             # Represent the date & end time of the file as YYYYMMDD-HHMM\n",
    "# #             file_end_date_time = '-'.join([archive_date, file_end_time])\n",
    "\n",
    "# #             # Grab the file ID\n",
    "# #             file_uri = row.find('a')['href'].split('/')[-1]\n",
    "\n",
    "# #             # Put the file date/time and URL leaf (as a list) into the list\n",
    "# #             table_entry_builder.append([file_uri, file_end_date_time])\n",
    "        \n",
    "# #         return table_entry_builder\n",
    "\n",
    "#     def __get_mp3_urls(self):\n",
    "#         # Get the first page\n",
    "#         parent.last_page_request_time = timer()\n",
    "#         browser.get('https://m.broadcastify.com/archives/id/' + self.table_entries[0][1])\n",
    "\n",
    "\n",
    "# #         # Log in so we can download files\n",
    "# #         ## Store the fields for username + password\n",
    "# #         username_field = browser.find_element_by_id(\"signinSrEmail\") \n",
    "# #         password_field = browser.find_element_by_id(\"signinSrPassword\")\n",
    "\n",
    "# #         ## Type username + password, and hit \"enter\"\n",
    "# #         username_field.send_keys(USERNAME)\n",
    "# #         password_field.send_keys(PASSWORD)\n",
    "# #         password_field.send_keys(Keys.RETURN)\n",
    "\n",
    "# #         ## Wait for login to complete\n",
    "# #         browser.implicitly_wait(2)\n",
    "\n",
    "#         # Get the filepath for the mp3 archive\n",
    "#         self.table_entries[0].append(get_mp3_path(BeautifulSoup(browser.page_source, 'lxml')))\n",
    "\n",
    "#         for row in self.table_entries[1:11]:\n",
    "#            # Wait until some time has passed, out of courtesy\n",
    "#             while not courtesy_wait(parent.last_page_request_time): pass\n",
    "\n",
    "#             # Get the next archive page, recording the time\n",
    "#             browser.get('https://m.broadcastify.com/archives/id/' + row[1])\n",
    "#             parent.last_page_request_time = timer()\n",
    "\n",
    "#             # Get the filepath for the mp3 archive\n",
    "#             row.append(get_mp3_path(BeautifulSoup(browser.page_source, 'lxml')))\n",
    "        \n",
    "#     def __format_archive_date(self):\n",
    "\n",
    "#         # Extract the day, month, and year of the data displayed on the page\n",
    "#         day = self.soup.find('td', {'class': 'active day'}).text\n",
    "#         month, year = self.soup.find('th', {'class': 'datepicker-switch'}).text.split()\n",
    "\n",
    "#         # Format the date as YYYYMMDD\n",
    "#         formatted_date = str(year) + str(MONTHS.index(month)).zfill(2) + day.zfill(2)\n",
    "\n",
    "#         return formatted_date\n",
    "    \n",
    "#     def __time_to_hhmm(self, s):\n",
    "#         # More details, since it's a one-line method and this isn't freaking codewars:\n",
    "#             # strptime converts the string to datetime \n",
    "#                 # see https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior\n",
    "#                 # and https://stackoverflow.com/questions/19229190/convert-12-hour-into-24-hour-times\n",
    "#             # first split separates YYYY-MM-DD from HH:MM\n",
    "#             # second split gets rid of the colon between HH & MM\n",
    "#             # join puts HHMM together\n",
    "#         # Converts a string representing a time in HH:MM AM/PM format to a string in 24-hr HHMM\n",
    "#         return ''.join(str(datetime.strptime(s, '%I:%M %p')).split(' ')[-1].split(':')[:2])\n",
    "            \n",
    "#     def __repr__(self):\n",
    "#         return (f'ArchiveTimesTable({len(self.table_entries)} entries)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEST_FEED_ID = '18812'\n",
    "TEST_FEED_ID = '10904'\n",
    "TEST_DOWNLOAD_ID = '774426456'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = BroadcastifyArchive(TEST_FEED_ID, USERNAME, PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive build complete.\n",
      "BroadcastifyArchive(26 ArchiveEntries; from 2019-07-31 13:20:00 to 2019-07-31 00:24:00. https://m.broadcastify.com/archives/feed/10904)\n"
     ]
    }
   ],
   "source": [
    "archive.build(days_back=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_att_date_for_filename(date):\n",
    "        # Format the ArchiveEntry end time as YYYYMMDD-HHMM\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "        day = date.day\n",
    "        hour = date.hour\n",
    "        minute = date.minute\n",
    "        \n",
    "        formatted_date = '-'.join([str(year) + str(month).zfill(2) + str(day).zfill(2), \n",
    "                                   str(hour).zfill(2) + str(minute).zfill(2)])\n",
    "\n",
    "        return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArchiveEntry(feed_id='10904', file_uri='775072119', file_end_datetime=datetime.datetime(2019, 7, 31, 13, 20), mp3_url='http://garchives1.broadcastify.com/10904/20190731/201907311150-628901-10904.mp3'),\n",
       " ArchiveEntry(feed_id='10904', file_uri='775066724', file_end_datetime=datetime.datetime(2019, 7, 31, 12, 50), mp3_url='http://garchives1.broadcastify.com/10904/20190731/201907311120-81567-10904.mp3'),\n",
       " ArchiveEntry(feed_id='10904', file_uri='775059693', file_end_datetime=datetime.datetime(2019, 7, 31, 12, 20), mp3_url='http://garchives1.broadcastify.com/10904/20190731/201907311050-722987-10904.mp3')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive.entries[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = DownloadNavigator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads complete.\n"
     ]
    }
   ],
   "source": [
    "dn.get_archive_mp3s(archive.entries[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "555px",
    "left": "918px",
    "right": "20px",
    "top": "41px",
    "width": "514px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
