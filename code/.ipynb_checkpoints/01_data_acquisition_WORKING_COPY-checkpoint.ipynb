{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import clear_output\n",
    "from time import time as timer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import wait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "## For abandoned approaches\n",
    "# import sys\n",
    "# import time\n",
    "# from PyQt5.QtWidgets import QApplication\n",
    "# from PyQt5.QtCore import QUrl\n",
    "# from PyQt5.QtWebEngineWidgets import QWebEngineView\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST_WAIT_SECS = 3\n",
    "# DATA_PATH = '../⁨data/⁨test_data/test_mp3⁩/' <-- This one has NPC from copying from Finder...\n",
    "    # only discovered b/c google's search for this page https://www.reddit.com/r/learnpython/comments/aafp0x/how_should_i_open_a_txt_file_that_downloaded/\n",
    "    # had the npc's encoded in the search result (https://www.google.com/search?q=%5BErrno+2%5D+No+such+file+or+directory%3A+%27..%2F%5Cu2068&rlz=1C5CHFA_enUS845US845&oq=%5BErrno+2%5D+No+such+file+or+directory%3A+%27..%2F%5Cu2068&aqs=chrome..69i57.4788j0j7&sourceid=chrome&ie=UTF-8)\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "TEST_MP3_OUT_PATH = DATA_PATH + 'test_data/test_mp3/'\n",
    "\n",
    "TEST_FEED_ID = '18812'\n",
    "\n",
    "USERNAME = 'cwchiu'\n",
    "PASSWORD = 'datascientists'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-level functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_hhmm(s):\n",
    "    # Converts a string representing a time in HH:MM AM/PM format to a string in 24-hr HHMM\n",
    "    return ''.join(str(datetime.strptime(s, '%I:%M %p')).split(' ')[-1].split(':')[:2])\n",
    "\n",
    "    # More details, since it's a one-line and this isn't freaking codewars:\n",
    "        # strptime converts the string to datetime \n",
    "            # see https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior\n",
    "            # and https://stackoverflow.com/questions/19229190/convert-12-hour-into-24-hour-times\n",
    "        # first split separates YYYY-MM-DD from HH:MM\n",
    "        # second split gets rid of the colon between HH & MM\n",
    "        # join puts HHMM together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_archive_times_table(archive_page_soup):\n",
    "    \"\"\"\n",
    "    Generates a list of Broadcastify archive file information from\n",
    "    the `archiveTimes` table on a feed's archive page. The list includes\n",
    "    two elements:\n",
    "        - Date and end time of the transmission in the format YYYYMMDD-HHMM,\n",
    "          on a 24-hour clock\n",
    "        - The unique ID for the file, which can be used to find the file's\n",
    "          individual download page\n",
    "         \n",
    "    Parameters\n",
    "    ----------\n",
    "    archive_page_soup : bs4.BeautifulSoup\n",
    "        A BeautifulSoup object containing the feed archive page source code, \n",
    "        e.g. from https://m.broadcastify.com/archives/feed/[feed_id]\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    # Set up a blank list to return\n",
    "    files_to_download = []\n",
    "    \n",
    "    # Isolate the `archive_times` table body\n",
    "    archive_times = archive_page_soup.find('table', attrs={'id': 'archiveTimes'}).find('tbody')\n",
    "    \n",
    "    # Find the date of transmission of the archived files\n",
    "    archive_date = get_archive_date(archive_page_soup)\n",
    "    \n",
    "    # Loop through all rows of the table\n",
    "    for row in archive_times.find_all('tr'):\n",
    "        file = []\n",
    "        file_info = []\n",
    "\n",
    "        # Grab the end time, contained in the row's second <td> tag\n",
    "        file_end_time = time_to_hhmm(row.find_all('td')[1].text) \n",
    "\n",
    "        # Represent the date & end time of the file as YYYYMMDD-HHMM\n",
    "        file_end_date_time = '-'.join([archive_date, file_end_time])\n",
    "\n",
    "        # Grab the file ID\n",
    "        file_id = row.find('a')['href'].split('/')[-1]\n",
    "\n",
    "        # Put the file date/time and URL leaf (as a list) into the list\n",
    "        files_to_download.append([file_end_date_time, file_id])\n",
    "\n",
    "    return files_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archive_date(archive_page_soup):\n",
    "    MONTHS = ['','January', 'February', 'March',\n",
    "          'April', 'May', 'June',\n",
    "          'July', 'August', 'September',\n",
    "          'October', 'November', 'December']\n",
    "    \n",
    "    # Extract the day, month, and year of the data displayed on the page\n",
    "    day = archive_page_soup.find('td', {'class': 'active day'}).text\n",
    "    month, year = archive_page_soup.find('th', {'class': 'datepicker-switch'}).text.split()\n",
    "    \n",
    "    # Format the date as YYYYMMDD\n",
    "    formatted_date = str(year) + str(MONTHS.index(month)).zfill(2) + day.zfill(2)\n",
    "\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mp3_path(download_page_soup):\n",
    "    # Get the filepath for the mp3 archive\n",
    "    return download_page_soup.find('a', {'href': re.compile('.mp3')}).attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def courtesy_wait(last_hit, wait_time=REQUEST_WAIT_SECS):\n",
    "    return time.time() - last_hit >= wait_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKING VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape feed archive page with `selenium`\n",
    "\n",
    "Need to use browser emulation because the page elements we need are rendered in JavaScript after the page loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a browser\n",
    "browser = webdriver.Chrome('../assets/chromedriver')\n",
    "\n",
    "# Open the feed archive page\n",
    "browser.get('https://m.broadcastify.com/archives/feed/' + TEST_FEED_ID)\n",
    "\n",
    "## Wait for page to render\n",
    "element = WebDriverWait(browser, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"cursor-link\")))\n",
    "\n",
    "# Capture page content as a BSoup\n",
    "archive_page_soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "\n",
    "# Quit the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the page's `archiveTimes` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['20190725-1725', '773188378'],\n",
       " ['20190725-1655', '773180493'],\n",
       " ['20190725-1625', '773175133'],\n",
       " ['20190725-1555', '773168047'],\n",
       " ['20190725-1525', '773162347'],\n",
       " ['20190725-1455', '773153902'],\n",
       " ['20190725-1426', '773147529'],\n",
       " ['20190725-1356', '773142063'],\n",
       " ['20190725-1326', '773135498'],\n",
       " ['20190725-1256', '773127936'],\n",
       " ['20190725-1226', '773121041'],\n",
       " ['20190725-1156', '773113998'],\n",
       " ['20190725-1127', '773107706'],\n",
       " ['20190725-1057', '773100451'],\n",
       " ['20190725-1027', '773093817'],\n",
       " ['20190725-0957', '773088542'],\n",
       " ['20190725-0927', '773081457'],\n",
       " ['20190725-0857', '773074135'],\n",
       " ['20190725-0828', '773066956'],\n",
       " ['20190725-0758', '773062167'],\n",
       " ['20190725-0728', '773054298'],\n",
       " ['20190725-0658', '773047854'],\n",
       " ['20190725-0628', '773040908'],\n",
       " ['20190725-0558', '773033921'],\n",
       " ['20190725-0529', '773026897'],\n",
       " ['20190725-0459', '773020840'],\n",
       " ['20190725-0429', '773014535'],\n",
       " ['20190725-0359', '773007881'],\n",
       " ['20190725-0329', '773001398'],\n",
       " ['20190725-0259', '772994266'],\n",
       " ['20190725-0230', '772987012'],\n",
       " ['20190725-0200', '772981145'],\n",
       " ['20190725-0130', '772974700'],\n",
       " ['20190725-0100', '772968172'],\n",
       " ['20190725-0030', '772961436'],\n",
       " ['20190725-0000', '772954328']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_info = parse_archive_times_table(archive_page_soup)\n",
    "archive_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get `mp3` paths for archived files\n",
    "\n",
    "Login required to access pages in `/archives/id/`. \n",
    "\n",
    "Consider re-implementing with a `requests.Session`...not sure if it would be faster or by how much..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a browser\n",
    "browser = webdriver.Chrome('../assets/chromedriver')\n",
    "\n",
    "# Get the first page\n",
    "browser.get('https://m.broadcastify.com/archives/id/' + archive_info[0][1])\n",
    "last_page_request_time = time.time()\n",
    "\n",
    "# Log in so we can download files\n",
    "## Store the fields for username + password\n",
    "username = browser.find_element_by_id(\"signinSrEmail\") \n",
    "password = browser.find_element_by_id(\"signinSrPassword\")\n",
    "\n",
    "## Type username + password, and hit \"enter\"\n",
    "username.send_keys(USERNAME)\n",
    "password.send_keys(PASSWORD)\n",
    "password.send_keys(Keys.RETURN)\n",
    "\n",
    "## Wait for login to complete\n",
    "browser.implicitly_wait(2)\n",
    "\n",
    "# Get the filepath for the mp3 archive\n",
    "archive_info[0].append(get_mp3_path(BeautifulSoup(browser.page_source, 'lxml')))\n",
    "\n",
    "for row in archive_info[1:]:\n",
    "   # Wait until some time has passed, out of courtesy\n",
    "    while not courtesy_wait(last_page_request_time): pass\n",
    "\n",
    "    # Get the next archive page, recording the time\n",
    "    browser.get('https://m.broadcastify.com/archives/id/' + row[1])\n",
    "    last_page_request_time = time.time()\n",
    "    \n",
    "    # Get the filepath for the mp3 archive\n",
    "    row.append(get_mp3_path(BeautifulSoup(browser.page_source, 'lxml')))\n",
    "\n",
    "# Quit the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['20190725-1725',\n",
       "  '773188378',\n",
       "  'http://garchives1.broadcastify.com/18812/20190725/201907251555-210863-18812.mp3'],\n",
       " ['20190725-1655',\n",
       "  '773180493',\n",
       "  'http://garchives1.broadcastify.com/18812/20190725/201907251555-210863-18812.mp3'],\n",
       " ['20190725-1625',\n",
       "  '773175133',\n",
       "  'http://garchives1.broadcastify.com/18812/20190725/201907251525-847590-18812.mp3'],\n",
       " ['20190725-1555',\n",
       "  '773168047',\n",
       "  'http://garchives1.broadcastify.com/18812/20190725/201907251455-596093-18812.mp3'],\n",
       " ['20190725-1525',\n",
       "  '773162347',\n",
       "  'http://garchives1.broadcastify.com/18812/20190725/201907251425-507163-18812.mp3']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_info[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download `.mp3` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://markhneedham.com/blog/2018/07/15/python-parallel-download-files-requests/\n",
    "\n",
    "def fetch_mp3(entry):\n",
    "    path, uri = entry\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        r = requests.get(uri, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(path, 'wb') as f:\n",
    "                for chunk in r:\n",
    "                    f.write(chunk)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = timer()\n",
    "last_page_request_time = time.time() - REQUEST_WAIT_SECS\n",
    "\n",
    "for file in archive_info:\n",
    "    file_date = file[0]\n",
    "    file_id = file[1]\n",
    "    url = file[2]\n",
    "    \n",
    "    # Build the filename we'll store the downloaded .mp3 under\n",
    "    file_name = TEST_MP3_OUT_PATH + '-'.join([TEST_FEED_ID, file_date]) + '.mp3'\n",
    "\n",
    "    print(f'Downloading {archive_info.index(file) + 1} of {len(archive_info)}')\n",
    "    print(f'\\tfrom {url}')\n",
    "    print(f'\\tto {file_name}')\n",
    "    \n",
    "    # Wait until some time has passed, out of courtesy\n",
    "    while not courtesy_wait(last_page_request_time): pass\n",
    "\n",
    "    fetch_mp3([file_name, url])\n",
    "    last_page_request_time = time.time()\n",
    "    clear_output(wait=True)\n",
    "        # h/t @schmitty\n",
    "    \n",
    "print('**** Downloads complete! ****')\n",
    "print(f'Elapsed Time: {round(timer() - start, 4)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape `.mp3` download page with `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_data = {\n",
    "    'username': 'cwchiu',\n",
    "    'password': 'datascientists',\n",
    "    'action': 'auth',\n",
    "    'redirect': '/'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) ' +\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/' +\n",
    "                  '75.0.3770.142 Safari/537.36'\n",
    "}\n",
    "\n",
    "login_url = 'https://m.broadcastify.com/login/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h/t to Indian Pythonista for the info about request.Session\n",
    "    # https://www.youtube.com/watch?v=fmf_y8zpOgA\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.post(login_url, data=login_data, headers=headers)\n",
    "    r = s.get('https://m.broadcastify.com/archives/id/772540078')\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    a_list = soup.find('a', {'href': re.compile('.mp3')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape feed archive page with `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atlanta Police Zone 5 and Fire Dispatch**\n",
    "\n",
    "Broadcasting Atlanta PD Zone 5 dispatch and Atlanta Fire and Rescue dispatch from Uniden Home Patrol and Windows 10 laptop. Limited to dispatch only. Adding allowed tac sub-channels will talk-over the dispatch channels.\n",
    "\n",
    "https://www.broadcastify.com/listen/feed/18812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed archives = https://m.broadcastify.com/archives/feed/18812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h/t to Indian Pythonista for the info about request.Session\n",
    "    # https://www.youtube.com/watch?v=fmf_y8zpOgA\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.post(login_url, data=login_data, headers=headers)\n",
    "    start_time = time.time()\n",
    "    r = s.get('https://m.broadcastify.com/archives/feed/18812')\n",
    "    while time.time() - start_time <= 5: pass\n",
    "    soup = BeautifulSoup(r.text, 'html5lib')\n",
    "    a_list = soup.find_all('a', {'title': 'Download audio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape feed archive page with `QWebEngineView` custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Render(QWebEngineView):\n",
    "        def __init__(self, url):\n",
    "            self.html = None\n",
    "            self.app = QApplication(sys.argv)\n",
    "            QWebEngineView.__init__(self)\n",
    "            self.loadFinished.connect(self._loadFinished)\n",
    "            #self.setHtml(html)\n",
    "            self.load(QUrl(url))\n",
    "            self.app.exec_()\n",
    "\n",
    "        def _loadFinished(self, result):\n",
    "            # This is an async call, you need to wait for this\n",
    "            # to be called before closing the app\n",
    "            self.page().toHtml(self._callable)\n",
    "\n",
    "        def _callable(self, data):\n",
    "            self.html = data\n",
    "            # Data has been stored, it's safe to quit the app\n",
    "            self.app.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(source_url):\n",
    "    \"\"\"Fully render HTML, JavaScript and all.\"\"\"\n",
    "    return Render(source_url).html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://m.broadcastify.com/archives/feed/18812\"\n",
    "print(render(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# for file in archive_info[0]:\n",
    "#     file_id = file[1]\n",
    "#     url = file[2]\n",
    "    \n",
    "#     print(f'Downloading {file_id}')\n",
    "#     os.system(f'wget {url}')\n",
    "    \n",
    "    \n",
    "#     'wget --http-user={USERNAME} --http-password={PASSWORD} {url}'\n",
    "#     '--user-agent'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "677px",
    "left": "922px",
    "right": "20px",
    "top": "8px",
    "width": "496px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
